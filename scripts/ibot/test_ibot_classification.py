#!/usr/bin/env python3
"""
Quick test script for iBOT classification
Tests if the classification script can load models and run without errors.
"""

import os
import sys
import torch

# Add paths for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_ibot_models():
    """Test if iBOT models can be loaded."""
    print("ğŸ§ª Testing iBOT model loading...")
    
    models_dir = "models/ibot"
    if not os.path.exists(models_dir):
        print(f"âŒ Models directory not found: {models_dir}")
        return False
    
    # List available models
    print(f"ğŸ“ Available models in {models_dir}:")
    for file in os.listdir(models_dir):
        file_path = os.path.join(models_dir, file)
        if file.endswith('.pth'):
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            print(f"   â€¢ {file} ({size_mb:.1f} MB)")
    
    # Test loading the main model
    main_model_path = os.path.join(models_dir, "final_model.pth")
    if os.path.exists(main_model_path):
        print(f"\nâœ… Testing main model: {main_model_path}")
        try:
            checkpoint = torch.load(main_model_path, map_location='cpu')
            print(f"   âœ… Model loaded successfully")
            
            # Check checkpoint structure
            if 'student' in checkpoint:
                print(f"   ğŸ“Š Checkpoint type: iBOT SSL training")
                student_keys = list(checkpoint['student'].keys())
                print(f"   ğŸ“‹ Student keys: {len(student_keys)} keys")
                print(f"   ğŸ”‘ Sample keys: {student_keys[:5]}")
            elif 'model_state_dict' in checkpoint:
                print(f"   ğŸ“Š Checkpoint type: Fine-tuned model")
            else:
                print(f"   ğŸ“Š Checkpoint type: Direct model weights")
            
            return True
            
        except Exception as e:
            print(f"   âŒ Failed to load model: {e}")
            return False
    else:
        print(f"âŒ Main model not found: {main_model_path}")
        return False

def test_data_availability():
    """Test if CIFAR-10 data is available."""
    print("\nğŸ§ª Testing data availability...")
    
    data_dir = "data/cifar10_splits"
    if not os.path.exists(data_dir):
        print(f"âŒ Data directory not found: {data_dir}")
        return False
    
    required_files = ['train_images.npy', 'train_labels.npy', 
                     'val_images.npy', 'val_labels.npy',
                     'test_images.npy', 'test_labels.npy']
    
    missing_files = []
    for file in required_files:
        file_path = os.path.join(data_dir, file)
        if not os.path.exists(file_path):
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ Missing data files: {missing_files}")
        return False
    
    print(f"âœ… All required data files found in {data_dir}")
    
    # Check data sizes
    for file in required_files:
        file_path = os.path.join(data_dir, file)
        size_mb = os.path.getsize(file_path) / (1024 * 1024)
        print(f"   â€¢ {file}: {size_mb:.1f} MB")
    
    return True

def test_imports():
    """Test if all required packages can be imported."""
    print("\nğŸ§ª Testing package imports...")
    
    try:
        import torch
        print(f"   âœ… PyTorch: {torch.__version__}")
    except ImportError as e:
        print(f"   âŒ PyTorch: {e}")
        return False
    
    try:
        import timm
        print(f"   âœ… timm: {timm.__version__}")
    except ImportError as e:
        print(f"   âŒ timm: {e}")
        return False
    
    try:
        import numpy as np
        print(f"   âœ… NumPy: {np.__version__}")
    except ImportError as e:
        print(f"   âŒ NumPy: {e}")
        return False
    
    try:
        from sklearn.metrics import accuracy_score
        print(f"   âœ… scikit-learn: available")
    except ImportError as e:
        print(f"   âŒ scikit-learn: {e}")
        return False
    
    return True

def main():
    """Run all tests."""
    print("ğŸš€ iBOT Classification Test Suite")
    print("="*50)
    
    tests = [
        ("Package Imports", test_imports),
        ("Data Availability", test_data_availability),
        ("iBOT Models", test_ibot_models)
    ]
    
    results = {}
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ Test failed with error: {e}")
            results[test_name] = False
    
    # Summary
    print(f"\n{'='*50}")
    print("ğŸ“Š TEST SUMMARY")
    print("="*50)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{test_name}: {status}")
        if not passed:
            all_passed = False
    
    print(f"\nğŸ¯ Overall: {'âœ… ALL TESTS PASSED' if all_passed else 'âŒ SOME TESTS FAILED'}")
    
    if all_passed:
        print("\nğŸš€ Ready to run iBOT classification!")
        print("ğŸ’¡ You can now run:")
        print("   â€¢ Linear probing: python scripts/ibot/ibot_classification.py --mode linear_probe")
        print("   â€¢ Nonlinear probing: python scripts/ibot/ibot_classification.py --mode nonlinear_probe")
        print("   â€¢ Fine-tuning: python scripts/ibot/ibot_classification.py --mode fine_tune")
    else:
        print("\nâš ï¸ Please fix the failed tests before running classification.")

if __name__ == "__main__":
    main()

